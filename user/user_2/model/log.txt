/home/asd36952/labeling/code/rnn.py:142: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  encoder_output_forward, (encoder_hidden_forward, encoder_cell_forward) = self.encoder_forward(sentence_embedding, (encoder_hidden_forward, encoder_cell_forward))
/home/asd36952/labeling/code/rnn.py:143: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  encoder_output_backward, (encoder_hidden_backward, encoder_cell_backward) = self.encoder_backward(reversed_sentence_embedding, (encoder_hidden_backward, encoder_cell_backward))
/home/asd36952/labeling/code/rnn.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  att = nn.Softmax()(torch.matmul(encoder_output_list[batch_idx], encoder_cell[batch_idx]))
/home/asd36952/labeling/code/rnn.py:167: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  output = nn.Softmax()(self.fc3(output))
1.9420974
1.9269942
1.9301145
1.9287148
1.907825
(0.0, 0.0, 0.0)
1.9088118
1.9027185
1.9110928
1.8917549
1.8827145
(0.0, 0.0, 0.0)
1.8826776
1.8505228
1.8633727
1.8669066
1.8630567
(0.0, 0.0, 0.0)
1.8292983
1.8375938
1.8117213
1.8396167
1.8256762
(0.0, 0.0, 0.0)
1.8109105
1.779662
1.8429537
1.7612574
1.7526011
(0.0, 0.0, 0.0)
1.7619864
1.758614
1.7744492
1.7299204
1.7405243
(0.0, 0.0, 0.0)
1.7070594
1.7144747
1.7069359
1.6586012
1.6532986
(0.0, 0.0, 0.0)
1.7367761
1.7032008
1.6969697
1.7672921
1.672972
(0.0, 0.0, 0.0)
1.6352732
1.6540606
1.6360193
1.5898885
1.6673701
(0.0, 0.0, 0.0)
1.7079587
1.632983
1.5933132
1.6811159
1.6879503
(0.0, 0.0, 0.0)
1.7668533
1.652111
1.7090005
1.6061296
1.6798254
(0.0, 0.0, 0.0)
1.694661
1.6269441
1.4874716
1.6108382
1.5861835
(1.0, 0.003134796238244514, 0.00625)
1.6427186
1.6156077
1.5868974
1.6984332
1.5721875
(1.0, 0.003134796238244514, 0.00625)
1.6340016
1.5351875
1.6145537
1.6378179
1.6446643
(1.0, 0.003134796238244514, 0.00625)
1.638319
1.6383915
1.5680481
1.6906531
1.5406501
(1.0, 0.003134796238244514, 0.00625)
1.6307485
1.6677176
1.5719395
1.6138048
1.5846643
(0.3333333333333333, 0.003134796238244514, 0.006211180124223602)
1.584878
1.4825847
1.5828843
1.5859456
1.5538001
(0.5, 0.006269592476489028, 0.012383900928792569)
1.4880697
1.587661
1.4938579
1.6308882
1.5468583
(0.3333333333333333, 0.009404388714733543, 0.018292682926829267)
1.5556452
1.5734438
1.5978189
1.5076103
1.5467917
(0.2727272727272727, 0.009404388714733543, 0.01818181818181818)
1.5910752
1.5650136
1.4687254
1.509419
1.5644886
(0.23809523809523808, 0.01567398119122257, 0.02941176470588235)
1.5484884
1.5081258
1.5487595
1.4908348
1.502481
(0.2, 0.01567398119122257, 0.02906976744186046)
1.4986546
1.4905471
1.5905383
1.490603
1.4874301
(0.20833333333333334, 0.01567398119122257, 0.029154518950437316)
1.5206082
1.5653305
1.4921607
1.5142202
1.4884055
(0.1875, 0.018808777429467086, 0.03418803418803419)
1.4635568
1.5682204
1.5047004
1.446822
1.5111625
(0.23529411764705882, 0.025078369905956112, 0.04532577903682719)
1.4865322
1.4868565
1.4877858
1.4172374
1.5197221
(0.2631578947368421, 0.03134796238244514, 0.05602240896358542)
1.4810143
1.4676822
1.4242553
1.3802903
1.3577015
(0.25, 0.03761755485893417, 0.06539509536784742)
1.3664048
1.3617985
1.4333453
1.461646
1.4671254
(0.32075471698113206, 0.05329153605015674, 0.0913978494623656)
1.3350289
1.4797028
1.4830096
1.3655039
1.4155176
(0.33962264150943394, 0.05642633228840126, 0.09677419354838711)
1.2798132
1.3523929
1.3845708
1.4723437
1.2749512
(0.3466666666666667, 0.08150470219435736, 0.1319796954314721)
1.3662256
1.3615015
1.4105824
1.3564723
1.340112
(0.35135135135135137, 0.08150470219435736, 0.13231552162849872)
1.2860144
1.3164992
1.2725832
1.2575672
1.4114594
(0.35443037974683544, 0.0877742946708464, 0.1407035175879397)
1.3120773
1.3132911
1.3003403
1.2808615
1.235928
(0.35714285714285715, 0.09404388714733543, 0.14888337468982632)
1.2002286
1.306283
1.4865457
1.2909083
1.3087056
(0.3595505617977528, 0.10031347962382445, 0.1568627450980392)
1.1507399
1.1733642
1.2154448
1.2881258
1.2568703
(0.32673267326732675, 0.10344827586206896, 0.15714285714285714)
1.1572133
1.172988
1.2142692
1.230711
1.1422782
(0.30392156862745096, 0.09717868338557993, 0.14726840855106887)
1.0676218
1.1852148
1.203111
1.2754813
1.0912974
(0.336, 0.13166144200626959, 0.1891891891891892)
1.1542284
1.1613336
1.216084
1.2405711
1.1106299
(0.325, 0.12225705329153605, 0.1776765375854214)
1.123385
1.1811506
1.1692135
1.0537176
1.0727285
(0.33587786259541985, 0.13793103448275862, 0.19555555555555554)
1.1172404
1.1613393
1.0110673
0.9904036
1.1623385
(0.312, 0.12225705329153605, 0.17567567567567566)
1.0423124
1.1633079
1.1845187
1.0766673
1.1730723
(0.3, 0.12225705329153605, 0.17371937639198215)
1.095722
1.0521808
0.9990599
1.1743369
1.0537353
(0.31724137931034485, 0.14420062695924765, 0.1982758620689655)
1.0491452
0.9948099
0.9834806
1.0024391
0.9269942
(0.34375, 0.1724137931034483, 0.22964509394572025)
1.0405692
0.8988888
1.0733404
0.98315334
0.87641424
(0.3051948051948052, 0.14733542319749215, 0.19873150105708245)
1.0115618
0.9956708
0.99381864
0.9243361
0.9067866
(0.31901840490797545, 0.16300940438871472, 0.21576763485477177)
1.089474
1.0167346
0.83835465
1.0282173
0.97338057
(0.304635761589404, 0.14420062695924765, 0.19574468085106383)
0.9824109
0.9131356
0.91081774
0.8114169
0.9303837
(0.32934131736526945, 0.1724137931034483, 0.2263374485596708)
0.92192554
0.87426627
0.8232143
0.9010128
0.7597646
(0.29651162790697677, 0.15987460815047022, 0.20773930753564157)
0.91630507
0.9204505
0.8171946
0.7878735
1.0094275
(0.3333333333333333, 0.18495297805642633, 0.2379032258064516)
0.9355092
0.8571423
0.83753955
1.0986302
0.91230404
(0.3157894736842105, 0.16927899686520376, 0.2204081632653061)
0.81360257
0.86982036
0.9245566
0.8826542
0.9495296
(0.3045977011494253, 0.16614420062695925, 0.2150101419878296)
0.78618383
0.77938724
0.8105694
0.7730845
0.77666306
(0.3298429319371728, 0.1974921630094044, 0.2470588235294118)
0.86263645
0.9147055
0.81886506
0.7411238
0.7754389
(0.3160621761658031, 0.19122257053291536, 0.23828125)
0.684083
0.8089398
0.7817973
0.8523108
0.901566
(0.32673267326732675, 0.20689655172413793, 0.2533589251439539)
0.85673416
0.87514144
0.79810315
0.86910415
0.8309868
(0.3229166666666667, 0.19435736677115986, 0.24266144814090024)
0.86842513
0.81287575
0.8103882
0.88319093
0.85407126
(0.32323232323232326, 0.2006269592476489, 0.24758220502901354)
0.68225116
0.77203166
0.74039084
0.66861963
0.71856105
(0.3096446700507614, 0.19122257053291536, 0.23643410852713181)
0.68316615
0.79717493
0.77187145
0.7540215
0.73357254
(0.3283582089552239, 0.20689655172413793, 0.25384615384615383)
0.8606447
0.759393
0.68024445
0.68017197
0.67807436
(0.3147208121827411, 0.19435736677115986, 0.24031007751937986)
0.7044102
0.72114617
0.6482513
0.6230645
0.60407895
(0.3427230046948357, 0.22884012539184953, 0.2744360902255639)
0.6808667
0.70390517
0.6562097
0.6689223
0.6418116
(0.319047619047619, 0.21003134796238246, 0.25330812854442347)
0.6451127
0.77530044
0.64724445
0.7089989
0.77943623
(0.3317972350230415, 0.22570532915360503, 0.2686567164179105)
0.6257012
0.7098295
0.7245809
0.639704
0.776386
(0.3209302325581395, 0.21630094043887146, 0.2584269662921348)
0.7236124
0.64906263
0.7434382
0.68338114
0.6116327
(0.34375, 0.2413793103448276, 0.283609576427256)
0.6022334
0.6125082
0.61358166
0.60718036
0.6529628
(0.32075471698113206, 0.21316614420062696, 0.256120527306968)
0.613612
0.60798067
0.4450345
0.59935355
0.6115032
(0.33620689655172414, 0.2445141065830721, 0.2831215970961887)
0.6802666
0.5988429
0.5991792
0.5449862
0.64631224
(0.32, 0.22570532915360503, 0.2647058823529412)
0.5225502
0.61441594
0.54132736
0.54251313
0.55734336
(0.32599118942731276, 0.23197492163009403, 0.27106227106227104)
0.5927907
0.46454382
0.4966747
0.54790926
0.6645019
(0.31363636363636366, 0.21630094043887146, 0.2560296846011132)
0.6033926
0.5781239
0.5788534
Process ForkPoolWorker-38402:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 108, in worker
    task = get()
  File "/usr/lib/python3.5/multiprocessing/queues.py", line 342, in get
    with self._rlock:
  File "/usr/lib/python3.5/multiprocessing/synchronize.py", line 96, in __enter__
    return self._semlock.__enter__()
KeyboardInterrupt
Process ForkPoolWorker-38401:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 108, in worker
    task = get()
  File "/usr/lib/python3.5/multiprocessing/queues.py", line 343, in get
    res = self._reader.recv_bytes()
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Traceback (most recent call last):
  File "rnn_train.py", line 127, in <module>
    classifier.train(sentence, entity_position, filler_position, relation, label, args.batch_size, learning_rate = args.learning_rate, username = args.username)
  File "/home/asd36952/labeling/code/rnn.py", line 200, in train
    output = self(batch_sentence, batch_entity_position, batch_filler_position)[0]
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/asd36952/labeling/code/rnn.py", line 137, in forward
    sentence_embedding, reversed_sentence_embedding = self.embedding(batch_sentence, batch_entity_position, batch_filler_position)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/asd36952/labeling/code/rnn.py", line 93, in forward
    entity_position_embedding, reversed_entity_position_embedding = self.entity_position_embedding(entity_position)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/asd36952/labeling/code/rnn.py", line 59, in forward
    position_index = self.util.batch_position_to_index(position)
  File "/home/asd36952/labeling/code/util.py", line 62, in batch_position_to_index
    with mp.Pool() as p:
  File "/usr/lib/python3.5/multiprocessing/context.py", line 118, in Pool
    context=self.get_context())
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 168, in __init__
    self._repopulate_pool()
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 230, in _repopulate_pool
    self._pool.append(w)
KeyboardInterrupt
