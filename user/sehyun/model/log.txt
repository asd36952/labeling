2.1116655
/home/asd36952/labeling/code/rnn.py:142: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  encoder_output_forward, (encoder_hidden_forward, encoder_cell_forward) = self.encoder_forward(sentence_embedding, (encoder_hidden_forward, encoder_cell_forward))
/home/asd36952/labeling/code/rnn.py:143: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  encoder_output_backward, (encoder_hidden_backward, encoder_cell_backward) = self.encoder_backward(reversed_sentence_embedding, (encoder_hidden_backward, encoder_cell_backward))
/home/asd36952/labeling/code/rnn.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  att = nn.Softmax()(torch.matmul(encoder_output_list[batch_idx], encoder_cell[batch_idx]))
/home/asd36952/labeling/code/rnn.py:167: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  output = nn.Softmax()(self.fc3(output))
2.0943923
2.0771065
2.0609453
2.0454793
(0.0, 0.0, 0.0)
1.4022156
1.3917218
1.0647016
1.0565063
1.048497
(0.0, 0.0, 0.0)
1.0406909
1.0330191
1.0252819
1.0176
1.0096692
(0.0, 0.0, 0.0)
1.0012695
0.9926661
0.98396325
0.97472966
0.96483386
(0.0, 0.0, 0.0)
1.1260256
1.1169822
1.1079761
1.0985396
1.0894723
(0.0, 0.0, 0.0)
1.0805811
1.0720568
1.0634221
1.0549189
1.0461121
(0.0, 0.0, 0.0)
1.282898
1.2747124
1.2667614
1.2591738
1.2519029
(0.0, 0.0, 0.0)
1.244597
1.2371601
1.2295667
1.2216214
1.2133688
(0.0, 0.0, 0.0)
1.2050942
1.1967434
1.1885272
1.1801343
1.1715484
(0.0, 0.0, 0.0)
1.1629657
1.1541309
1.1450752
1.1357712
1.126297
(0.0, 0.0, 0.0)
1.1166669
1.1068308
1.0966917
1.086389
1.0759151
(0.0, 0.0, 0.0)
1.065586
1.0552396
1.0447694
1.186561
1.1757643
(0.0, 0.0, 0.0)
1.3139958
1.302985
1.2918913
1.2807071
1.2695458
(0.3333333333333333, 0.003134796238244514, 0.006211180124223602)
1.2584672
1.2475616
1.2368889
1.2264628
1.3976938
(0.1, 0.003134796238244514, 0.0060790273556231)
1.3781713
1.2438688
1.2288316
1.2144543
1.1107892
(0.0, 0.0, 0.0)
1.0988988
1.0878152
1.0770367
1.0664693
1.0558097
(0.0, 0.0, 0.0)
1.1303204
1.1156482
1.1033863
1.0926673
1.082213
(0.0, 0.0, 0.0)
1.0583054
1.0486687
1.039297
1.0298218
1.020216
(0.0, 0.0, 0.0)
1.0102228
0.9999646
0.98958313
0.9788211
0.96725035
(0.0, 0.0, 0.0)
1.0651392
1.0546077
1.0435424
1.0319881
1.0200698
(0.0, 0.0, 0.0)
1.0887071
1.0773822
1.0660458
1.0544214
1.0422268
(0.0, 0.0, 0.0)
1.0297083
1.0165687
1.00263
0.98724496
0.97131354
(0.0, 0.0, 0.0)
1.0484861
1.0335567
1.0177441
1.0016495
0.9868121
(0.0, 0.0, 0.0)
0.97256356
1.024962
1.0112476
0.9973898
0.9831685
(0.0, 0.0, 0.0)
0.9850904
0.97008526
0.9544166
0.93873084
0.9846603
(0.125, 0.006269592476489028, 0.011940298507462685)
0.9672543
0.94806004
0.9410146
0.9193413
0.8947627
(0.1702127659574468, 0.025078369905956112, 0.04371584699453552)
0.9162492
0.8842183
0.8545319
0.8391033
0.80834365
(0.19047619047619047, 0.03761755485893417, 0.06282722513089005)
0.7997761
0.77334696
0.753186
0.727303
0.72045577
(0.21348314606741572, 0.05956112852664577, 0.09313725490196077)
0.69488895
0.6686137
0.64334285
0.6206751
0.60006297
(0.19047619047619047, 0.06269592476489028, 0.09433962264150943)
0.5829041
0.5658478
0.55009973
0.5346489
0.634089
(0.1968503937007874, 0.07836990595611286, 0.11210762331838567)
0.6175323
0.5519487
0.51441
0.50033396
0.48668307
(0.1610738255033557, 0.07523510971786834, 0.10256410256410256)
0.4748527
0.46256962
0.45157075
0.43976817
0.4293858
(0.18235294117647058, 0.09717868338557993, 0.12678936605316973)
0.41921264
0.495538
0.47841674
0.46727082
0.45592517
(0.1774193548387097, 0.10344827586206896, 0.1306930693069307)
0.4464791
0.43546814
0.42680484
0.41755122
0.4096883
(0.2268041237113402, 0.13793103448275862, 0.17153996101364521)
0.4007588
0.3929879
0.38428667
0.3665833
0.3587607
(0.19704433497536947, 0.12539184952978055, 0.15325670498084293)
0.35295132
0.34484708
0.5356084
0.51875734
0.50221753
(0.2146341463414634, 0.13793103448275862, 0.1679389312977099)
0.51797116
0.4829203
0.46162295
0.44100618
0.42579246
(0.1796116504854369, 0.11598746081504702, 0.14095238095238097)
0.4161295
0.40559936
0.39303792
0.38557154
0.37151158
(0.14537444933920704, 0.10344827586206896, 0.12087912087912087)
0.36394936
0.35251743
0.34551775
0.333726
0.3276147
(0.1735159817351598, 0.11912225705329153, 0.14126394052044608)
0.31624365
0.31067753
0.30055243
0.29445857
0.2858091
(0.1581196581196581, 0.11598746081504702, 0.1338155515370705)
0.27862322
0.27013594
0.26566315
0.26114866
0.25655484
(0.17543859649122806, 0.12539184952978055, 0.14625228519195613)
0.25314263
0.2485007
0.24552892
0.24080324
0.23805134
(0.1646090534979424, 0.12539184952978055, 0.14234875444839853)
0.23334768
0.23101994
0.22613424
0.22426486
0.2191831
(0.18723404255319148, 0.13793103448275862, 0.1588447653429603)
0.21781081
0.2125679
0.21123981
0.2062575
0.20504229
(0.1646586345381526, 0.12852664576802508, 0.1443661971830986)
0.20026113
0.1990217
0.19456376
0.19326751
0.1891104
(0.17647058823529413, 0.13166144200626959, 0.15080789946140036)
0.18767676
0.18398982
0.18240118
0.17911662
0.17736337
(0.16334661354581673, 0.12852664576802508, 0.14385964912280705)
0.17440638
0.17258713
0.16988751
0.1680007
0.16554463
(0.17427385892116182, 0.13166144200626959, 0.15000000000000002)
0.16367134
0.16136876
0.15952665
0.15731189
0.1555273
(0.16334661354581673, 0.12852664576802508, 0.14385964912280705)
0.1533699
0.15159033
0.14953676
0.14777325
0.14573091
(0.17551020408163265, 0.13479623824451412, 0.1524822695035461)
0.14388429
0.14156975
0.13982835
0.1379444
0.13642678
(0.16996047430830039, 0.13479623824451412, 0.15034965034965037)
0.13455872
0.13310958
0.13134336
0.12995496
0.12814268
(0.18292682926829268, 0.14106583072100312, 0.15929203539823006)
0.1268968
0.12507705
0.12393583
0.12212093
0.12108637
(0.15953307392996108, 0.12852664576802508, 0.1423611111111111)
0.11929485
0.11837627
0.11652627
0.1157742
0.113864385
(0.18775510204081633, 0.14420062695924765, 0.16312056737588654)
0.11325382
0.11127666
0.11079919
0.10878226
0.10841057
(0.16796875, 0.13479623824451412, 0.14956521739130435)
0.106349304
0.10609776
0.1040075
0.10385279
0.101743855
(0.18326693227091634, 0.14420062695924765, 0.16140350877192983)
0.10166611
0.09957766
0.09953989
0.09745454
0.09745967
(0.16342412451361868, 0.13166144200626959, 0.14583333333333334)
0.095394395
0.09543568
0.09340834
0.09348521
0.09145558
(0.18253968253968253, 0.14420062695924765, 0.16112084063047286)
0.091570474
0.08958242
0.08973643
0.08774278
0.08793513
(0.16091954022988506, 0.13166144200626959, 0.14482758620689656)
0.08594472
0.08617245
0.08418081
0.08444307
0.082444
(0.1828793774319066, 0.14733542319749215, 0.16319444444444442)
0.082740396
0.08073629
0.081059776
0.07905853
0.07941024
(0.16216216216216217, 0.13166144200626959, 0.14532871972318337)
0.077416874
0.07777092
0.075771354
0.0761272
0.07414423
(0.18146718146718147, 0.14733542319749215, 0.16262975778546712)
0.074491054
0.07254141
0.072892025
0.07098608
0.07135636
(0.15708812260536398, 0.12852664576802508, 0.1413793103448276)
0.06944219
0.06983865
0.067913204
0.068284854
0.06636532
(0.18250950570342206, 0.15047021943573669, 0.16494845360824742)
0.06672673
0.06483428
0.065164834
0.063322775
0.06362332
(0.15708812260536398, 0.12852664576802508, 0.1413793103448276)
0.06183377
0.062120587
0.060360596
0.060615152
0.058885682
(0.18007662835249041, 0.14733542319749215, 0.1620689655172414)
0.05910391
0.057411477
0.05754
0.055919316
0.055996213
(0.16091954022988506, 0.13166144200626959, 0.14482758620689656)
0.054439574
0.05447232
0.052984387
0.052977875
0.05153448
(0.18076923076923077, 0.14733542319749215, 0.1623488773747841)
0.051481605
0.050084867
0.049980756
0.048627235
0.04843172
(0.1576923076923077, 0.12852664576802508, 0.14162348877374784)
0.04715672
0.04688974
0.0456929
0.045357645
0.044239596
(0.17692307692307693, 0.14420062695924765, 0.15889464594127808)
0.04385635
0.042784367
0.042353176
0.041350618
0.0408797
(0.1576923076923077, 0.12852664576802508, 0.14162348877374784)
0.03991527
0.0394123
0.038465664
0.038011946
0.037071496
(0.18007662835249041, 0.14733542319749215, 0.1620689655172414)
0.036640394
0.03570372
0.035389204
0.034395933
0.034163214
(0.16091954022988506, 0.13166144200626959, 0.14482758620689656)
0.033124343
0.032964718
0.031887025
0.031809036
0.03068946
(0.18181818181818182, 0.15047021943573669, 0.1646655231560892)
0.03067829
0.02953365
0.029596213
0.028430665
0.028541248
(0.15730337078651685, 0.13166144200626959, 0.14334470989761092)
0.027357165
0.027524345
0.02632873
0.02654291
0.025347793
(0.16544117647058823, 0.14106583072100312, 0.15228426395939085)
0.02562493
0.024402801
0.024743076
0.023499034
0.02389376
(0.15441176470588236, 0.13166144200626959, 0.14213197969543145)
0.02263229
0.02307884
0.021802971
0.022292957
0.021001998
(0.16176470588235295, 0.13793103448275862, 0.14890016920473773)
0.021537255
0.020237992
0.020806264
0.01950198
0.020104159
(0.15, 0.13166144200626959, 0.14023372287145242)
0.01879739
0.019433005
0.018112898
0.018788427
0.017457718
(0.15827338129496402, 0.13793103448275862, 0.1474036850921273)
0.01816554
0.016830495
0.01756346
0.016227849
0.016983204
(0.14285714285714285, 0.12539184952978055, 0.1335559265442404)
0.015650084
0.01641685
0.0150935585
0.015876647
0.014564246
(0.15579710144927536, 0.13479623824451412, 0.14453781512605043)
0.015353428
0.0140569955
0.014845912
0.013571245
0.014337654
(0.13427561837455831, 0.11912225705329153, 0.12624584717607973)
0.013108901
0.013862857
0.012660742
0.013407409
0.012231492
(0.14487632508833923, 0.12852664576802508, 0.1362126245847176)
0.012967595
0.011819985
0.012532259
0.011425755
0.012127472
(0.12631578947368421, 0.11285266457680251, 0.11920529801324503)
0.011044832
0.011721201
0.010690911
0.011341267
0.010344954
(0.13541666666666666, 0.12225705329153605, 0.128500823723229)
0.010959554
0.01002371
0.01057614
0.009736978
0.010246887
(0.1284722222222222, 0.11598746081504702, 0.12191103789126853)
0.009418203
0.009919186
0.009152843
0.009558133
0.008889478
(0.1292517006802721, 0.11912225705329153, 0.12398042414355628)
0.009265307
0.008599612
0.008976493
0.008346681
0.008666021
(0.12802768166089964, 0.11598746081504702, 0.12171052631578946)
0.008111919
0.008401863
0.007852608
0.008145265
0.0076092896
(0.13058419243986255, 0.11912225705329153, 0.12459016393442623)
0.007892644
0.007384182
0.007651912
0.0071531334
0.007422983
(0.12714776632302405, 0.11598746081504702, 0.12131147540983607)
0.0069346093
0.007199726
0.0067245844
0.0069805384
0.006522166
(0.125, 0.11598746081504702, 0.12032520325203251)
0.006768568
0.006327285
0.0065617277
0.0061396593
0.0063620177
(0.12758620689655173, 0.11598746081504702, 0.12151067323481117)
0.0059580784
0.0061675073
0.005783359
0.0059782173
0.0056138206
(0.12709030100334448, 0.11912225705329153, 0.12297734627831716)
0.0057946956
0.005449571
0.005616732
0.005290724
0.005444123
(0.125, 0.11285266457680251, 0.1186161449752883)
0.0051373118
0.0052772956
0.0049881903
0.005116064
0.004843885
(0.13, 0.12225705329153605, 0.1260096930533118)
0.0049603125
0.0047041066
0.004808135
0.0045690066
0.004661298
(0.12203389830508475, 0.11285266457680251, 0.1172638436482085)
Traceback (most recent call last):
  File "rnn_train.py", line 139, in <module>
    classifier.visualize(train_sentence, train_entity_position, train_filler_position, temp_relation + (["unlabeled"] * (len(train_relation) - len(temp_relation))), args.test_batch_size, args.username)
  File "/home/asd36952/labeling/code/rnn.py", line 285, in visualize
    tmp_output_list, tmp_vis_list, tmp_att_list = self(batch_sentence, batch_entity_position, batch_filler_position, max_len)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/asd36952/labeling/code/rnn.py", line 143, in forward
    encoder_output_backward, (encoder_hidden_backward, encoder_cell_backward) = self.encoder_backward(reversed_sentence_embedding, (encoder_hidden_backward, encoder_cell_backward))
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py", line 204, in forward
    output, hidden = func(input, self.all_weights, hx)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/_functions/rnn.py", line 385, in forward
    return func(input, *fargs, **fkwargs)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/autograd/function.py", line 328, in _do_forward
    flat_output = super(NestedIOFunction, self)._do_forward(*flat_input)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/autograd/function.py", line 350, in forward
    result = self.forward_extended(*nested_tensors)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/nn/_functions/rnn.py", line 294, in forward_extended
    cudnn.rnn.forward(self, input, hx, weight, output, hy)
  File "/home/asd36952/.local/lib/python3.5/site-packages/torch/backends/cudnn/rnn.py", line 295, in forward
    ctypes.c_void_p(fn.reserve.data_ptr()), fn.reserve.size(0)
KeyboardInterrupt
